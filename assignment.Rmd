---
title: "Keras - HW2 - MLT"
author: "Oscar Leal - Gerold Csendes"
date: "3/26/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Homework 2

```{r}
library(keras)
```

```{r}
library(here) 
library(grid)
library(magick)  # not absolutely necessary
library(data.table)
```

### 1. Fashion MNIST data (10 points)

Take the “Fashion MNIST dataset” where images of fashion items are to be classified in a similar manner to what we saw with handwritten digits (see more here). Images are in exactly the same format as we saw digits: 28x28 pixel grayscale images. The task is to build deep neural net models to predict image classes. The goal is to have as accurate classifier as possible: we are using accuracy as a measure of predictive power.

```{r}
fashion_mnist <- dataset_fashion_mnist()
x_train <- fashion_mnist$train$x
y_train <- fashion_mnist$train$y
x_test <- fashion_mnist$test$x
y_test <- fashion_mnist$test$y
```

- Show some example images from the data.

```{r, fig.width=2, fig.height=2}
show_mnist_image <- function(x) {
  image(1:28, 1:28, t(x)[,nrow(x):1],col=gray((0:255)/255)) 
}

show_mnist_image(x_train[15, , ])
```

The dataset Fashion MNIST dataset also has 10 possible outcomes (labels), so the story doesn't change much as the MNIST numbers worked with before.

- Normalize the data similarly to what we saw with MNIST.

```{r}
# reshape
x_train <- array_reshape(x_train, c(dim(x_train)[1], 784)) 
x_test <- array_reshape(x_test, c(dim(x_test)[1], 784)) 
# rescale
x_train <- x_train / 255
x_test <- x_test / 255

# one-hot encoding of the target variable
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
```

- Train a fully connected deep network to predict items.

```{r}
model <- keras_model_sequential() 
model %>% 
  layer_dense(units = 128, activation = 'relu', input_shape = c(784)) %>%
  layer_dense(units = 10, activation = 'softmax')
```

```{r}
summary(model)
# 1000480 = 784 (input features) * 128 (first layer nodes) + 128 (biases)
# 
```

```{r}
model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_adamax(),
  metrics = c('accuracy')
)
```

```{r}
# you can supply a validatio nset here in fit
history <- model %>% fit(
  x_train, y_train, 
  epochs = 30, batch_size = 128, 
  validation_split = 0.2
)
```

- Experiment with network architectures and settings (number of hidden layers, number of nodes, activation functions, dropout, etc.)

```{r}
second_model <- keras_model_sequential() 
second_model %>% 
  layer_dense(units = 128, input_shape = c(784)) %>%
  layer_activation_leaky_relu %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 64) %>%
  layer_activation_leaky_relu() %>% 
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 32) %>%
  layer_activation_relu() %>% 
  layer_dropout(rate = 0.15) %>%
  layer_dense(units = 10, activation = 'softmax')
```

```{r}
summary(second_model)
# 1000480 = 784 (input features) * 128 (first layer nodes) + 128 (biases)
# 
```

```{r}
second_model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)
```

```{r}
# you can supply a validatio nset here in fit
history_second <- second_model %>% fit(
  x_train, y_train, 
  epochs = 30, batch_size = 128, 
  validation_split = 0.2
)
```

- Explain what you have tried, what worked and what did not. Present a final model. Make sure that you use enough epochs so that the validation error starts flattening out - provide a plot about the training history (plot(history))

Seems like using the ADAMAX optimizer is not working well as the RMSprop optimizer.

```{r}
plot(history)
```

Even though the second model has less accuracy, the accuracy was actually 1% higher in the validation set which comparing to the first plot there is a huge gap between train and validation of 7% accuracy.

```{r}
plot(history_second)
```

- Evaluate the model on the test set. How does test error compare to validation error?

```{r}
model %>% evaluate(x_test, y_test)
```

```{r}
second_model %>% evaluate(x_test, y_test)
```

#### Despite the concern of the accuracies from the first model being so separate, the accuracy on the test set was better for the first model.

- Try building a convolutional neural network and see if you can improve test set performance.

```{r}
x_train <- fashion_mnist$train$x
y_train <- fashion_mnist$train$y
x_test <- fashion_mnist$test$x
y_test <- fashion_mnist$test$y

x_train <- array_reshape(x_train, c(nrow(x_train), 28, 28, 1))
x_test <- array_reshape(x_test, c(nrow(x_test), 28, 28, 1))

# rescale
x_train <- x_train / 255
x_test <- x_test / 255

# one-hot encoding of the target variable
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
```

```{r}
cnn_model <- keras_model_sequential() 
cnn_model %>% 
  layer_conv_2d(filters = 32,
                kernel_size = c(3, 3), 
                activation = 'relu',
                input_shape = c(28, 28, 1)) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_dropout(rate = 0.25) %>%
  layer_flatten() %>% 
  layer_dense(units = 16, activation = 'relu') %>% 
  layer_dense(units = 10, activation = 'softmax')
```

```{r}
summary(cnn_model)
```

```{r}
cnn_model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)
```

```{r}
cnn_history <- cnn_model %>% fit(
  x_train, y_train, 
  epochs = 10, batch_size = 128, 
  validation_split = 0.2
)
```

```{r}
cnn_model %>% evaluate(x_test, y_test)
```

Just like before, experiment with different network architectures, regularization techniques and present your findings

```{r}
second_cnn_model <- keras_model_sequential() 
second_cnn_model %>% 
  layer_conv_2d(filters = 64,
                kernel_size = c(3, 3),
                input_shape = c(28, 28, 1)) %>%
  layer_activation_leaky_relu() %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_dropout(rate = 0.25) %>%
  layer_flatten() %>% 
  layer_dense(units = 128) %>% 
  layer_activation_leaky_relu() %>% 
  layer_dense(units = 10, activation = 'softmax')
```

```{r}
summary(second_cnn_model)
```

```{r}
second_cnn_model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)
```

```{r}
second_cnn_history <- second_cnn_model %>% fit(
  x_train, y_train, 
  epochs = 15, batch_size = 128, 
  validation_split = 0.2
)
```

```{r}
second_cnn_model %>% evaluate(x_test, y_test)
```


### 2. Hot dog or not hot dog? (12 points)

In this problem you are going to predict if a certain image containing food is hot dog or is something else. Motivation for this comes from the comedy show Silicon Valley (see here).

The data can be found in the course repo and is originally downloaded from here.

- Pre-process data so that it is acceptable by Keras (set folder structure, bring images to the same size, etc).

- Estimate a convolutional neural network to predict if an image contains a hot dog or not. Evaluate your model on the test set.

- Could data augmentation techniques help with achieving higher predictive accuracy? Try some augmentations that you think make sense and compare

- Try to rely on some pre-built neural networks to aid prediction. Can you achieve a better performance using transfer learning for this problem?