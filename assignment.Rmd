---
title: "Keras - HW2 - MLT"
author: "Oscar Leal - Gerold Csendes"
date: "3/26/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Homework 2

```{r}
library(keras)

```

```{r}
library(keras)
library(here) 
library(grid)
library(magick)  # not absolutely necessary
library(data.table)
```

### 1. Fashion MNIST data (10 points)

Take the “Fashion MNIST dataset” where images of fashion items are to be classified in a similar manner to what we saw with handwritten digits (see more here). Images are in exactly the same format as we saw digits: 28x28 pixel grayscale images. The task is to build deep neural net models to predict image classes. The goal is to have as accurate classifier as possible: we are using accuracy as a measure of predictive power.


```{r, eval = FALSE}
fashion_mnist <- dataset_fashion_mnist()
x_train <- fashion_mnist$train$x
y_train <- fashion_mnist$train$y
x_test <- fashion_mnist$test$x
y_test <- fashion_mnist$test$y
```

- Show some example images from the data.

```{r, fig.width=2, fig.height=2}
show_mnist_image <- function(x) {
  image(1:28, 1:28, t(x)[,nrow(x):1],col=gray((0:255)/255)) 
}

show_mnist_image(x_train[20, , ])
```

- Train a fully connected deep network to predict items.

```{r}
# reshape
x_train <- array_reshape(x_train, c(dim(x_train)[1], 784)) 
x_test <- array_reshape(x_test, c(dim(x_test)[1], 784)) 
# rescale
x_train <- x_train / 255
x_test <- x_test / 255

# one-hot encoding of the target variable
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
```

- Normalize the data similarly to what we saw with MNIST.

```{r}
model <- keras_model_sequential() 
model %>% 
  layer_dense(units = 128, activation = 'relu', input_shape = c(784)) %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 10, activation = 'softmax')
```

```{r}
summary(model)
# 1000480 = 784 (input features) * 128 (first layer nodes) + 128 (biases)
# 
```

```{r}
model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)
```

```{r}
# you can supply a validatio nset here in fit
history <- model %>% fit(
  x_train, y_train, 
  epochs = 30, batch_size = 128, 
  validation_split = 0.3
)
```

- Experiment with network architectures and settings (number of hidden layers, number of nodes, activation functions, dropout, etc.)

- Explain what you have tried, what worked and what did not. Present a final model.

- Make sure that you use enough epochs so that the validation error starts flattening out - provide a plot about the training history (plot(history))

- Evaluate the model on the test set. How does test error compare to validation error?

- Try building a convolutional neural network and see if you can improve test set performance.

Just like before, experiment with different network architectures, regularization techniques and present your findings

### 2. Hot dog or not hot dog? (12 points)

In this problem you are going to predict if a certain image containing food is hot dog or is something else. Motivation for this comes from the comedy show Silicon Valley (see here).

The data can be found in the course repo and is originally downloaded from here.

- Pre-process data so that it is acceptable by Keras (set folder structure, bring images to the same size, etc).

- Estimate a convolutional neural network to predict if an image contains a hot dog or not. Evaluate your model on the test set.

- Could data augmentation techniques help with achieving higher predictive accuracy? Try some augmentations that you think make sense and compare

- Try to rely on some pre-built neural networks to aid prediction. Can you achieve a better performance using transfer learning for this problem?