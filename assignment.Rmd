---
title: "Keras - HW2 - MLT"
author: "Oscar Leal - Gerold Csendes"
date: "3/26/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Homework 2

```{r}
library(keras)
```

```{r}
library(here) 
library(grid)
library(magick)  # not absolutely necessary
library(data.table)
```

### 1. Fashion MNIST data (10 points)

Take the “Fashion MNIST dataset” where images of fashion items are to be classified in a similar manner to what we saw with handwritten digits (see more here). Images are in exactly the same format as we saw digits: 28x28 pixel grayscale images. The task is to build deep neural net models to predict image classes. The goal is to have as accurate classifier as possible: we are using accuracy as a measure of predictive power.

```{r}
fashion_mnist <- dataset_fashion_mnist()
x_train <- fashion_mnist$train$x
y_train <- fashion_mnist$train$y
x_test <- fashion_mnist$test$x
y_test <- fashion_mnist$test$y
```

- Show some example images from the data.

```{r, fig.width=2, fig.height=2}
show_mnist_image <- function(x) {
  image(1:28, 1:28, t(x)[,nrow(x):1],col=gray((0:255)/255)) 
}

show_mnist_image(x_train[15, , ])
```

The dataset Fashion MNIST dataset also has 10 possible outcomes (labels), so the story doesn't change much as the MNIST numbers worked with before.

- Normalize the data similarly to what we saw with MNIST.

```{r}
# reshape
x_train <- array_reshape(x_train, c(dim(x_train)[1], 784)) 
x_test <- array_reshape(x_test, c(dim(x_test)[1], 784)) 
# rescale
x_train <- x_train / 255
x_test <- x_test / 255

# one-hot encoding of the target variable
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
```

- Train a fully connected deep network to predict items.

```{r}
model <- keras_model_sequential() 
model %>% 
  layer_dense(units = 128, activation = 'relu', input_shape = c(784)) %>%
  layer_dense(units = 10, activation = 'softmax')
```

```{r}
summary(model)
# 1000480 = 784 (input features) * 128 (first layer nodes) + 128 (biases)
# 
```

```{r}
model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_adamax(),
  metrics = c('accuracy')
)
```

```{r}
# you can supply a validatio nset here in fit
history <- model %>% fit(
  x_train, y_train, 
  epochs = 30, batch_size = 128, 
  validation_split = 0.2
)
```

- Experiment with network architectures and settings (number of hidden layers, number of nodes, activation functions, dropout, etc.)

```{r}
second_model <- keras_model_sequential() 
second_model %>% 
  layer_dense(units = 128, input_shape = c(784)) %>%
  layer_activation_leaky_relu %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 64) %>%
  layer_activation_leaky_relu() %>% 
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 32) %>%
  layer_activation_relu() %>% 
  layer_dropout(rate = 0.15) %>%
  layer_dense(units = 10, activation = 'softmax')
```

```{r}
summary(second_model)
# 1000480 = 784 (input features) * 128 (first layer nodes) + 128 (biases)
# 
```

```{r}
second_model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)
```

```{r}
# you can supply a validatio nset here in fit
history_second <- second_model %>% fit(
  x_train, y_train, 
  epochs = 30, batch_size = 128, 
  validation_split = 0.2
)
```

- Explain what you have tried, what worked and what did not. Present a final model. Make sure that you use enough epochs so that the validation error starts flattening out - provide a plot about the training history (plot(history))

Seems like using the ADAMAX optimizer is not working well as the RMSprop optimizer.

```{r}
plot(history)
```

Even though the second model has less accuracy, the accuracy was actually 1% higher in the validation set which comparing to the first plot there is a huge gap between train and validation of 7% accuracy.

```{r}
plot(history_second)
```

- Evaluate the model on the test set. How does test error compare to validation error?

```{r}
model %>% evaluate(x_test, y_test)
```

```{r}
second_model %>% evaluate(x_test, y_test)
```

#### Despite the concern of the accuracies from the first model being so separate, the accuracy on the test set was better for the first model.

- Try building a convolutional neural network and see if you can improve test set performance.

```{r}
x_train <- fashion_mnist$train$x
y_train <- fashion_mnist$train$y
x_test <- fashion_mnist$test$x
y_test <- fashion_mnist$test$y

x_train <- array_reshape(x_train, c(nrow(x_train), 28, 28, 1))
x_test <- array_reshape(x_test, c(nrow(x_test), 28, 28, 1))

# rescale
x_train <- x_train / 255
x_test <- x_test / 255

# one-hot encoding of the target variable
y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)
```

```{r}
cnn_model <- keras_model_sequential() 
cnn_model %>% 
  layer_conv_2d(filters = 32,
                kernel_size = c(3, 3), 
                activation = 'relu',
                input_shape = c(28, 28, 1)) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_dropout(rate = 0.25) %>%
  layer_flatten() %>% 
  layer_dense(units = 16, activation = 'relu') %>% 
  layer_dense(units = 10, activation = 'softmax')
```

```{r}
summary(cnn_model)
```

```{r}
cnn_model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)
```

```{r}
cnn_history <- cnn_model %>% fit(
  x_train, y_train, 
  epochs = 10, batch_size = 128, 
  validation_split = 0.2
)
```

```{r}
cnn_model %>% evaluate(x_test, y_test)
```

Just like before, experiment with different network architectures, regularization techniques and present your findings

This time it will have an activation function of leaky RELU instead of the normal one, and 64 filters instead of 32, and another layer.

```{r}
second_cnn_model <- keras_model_sequential() 
second_cnn_model %>% 
  layer_conv_2d(filters = 64,
                kernel_size = c(3, 3),
                input_shape = c(28, 28, 1)) %>%
  layer_activation_leaky_relu() %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_dropout(rate = 0.25) %>%
  layer_flatten() %>% 
  layer_dense(units = 128) %>% 
  layer_activation_leaky_relu() %>% 
  layer_dense(units = 10, activation = 'softmax')
```

```{r}
summary(second_cnn_model)
```

```{r}
second_cnn_model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)
```

```{r}
second_cnn_history <- second_cnn_model %>% fit(
  x_train, y_train, 
  epochs = 15, batch_size = 128, 
  validation_split = 0.2
)
```

```{r}
second_cnn_model %>% evaluate(x_test, y_test)
```

Seems like the last CNN model is the outperformer with an accuracy of 90% on the test data.
From this last model we analyze its findings.

Matrix with mistakes:

```{r}
predicted_classes_test <- second_cnn_model %>% predict_classes(x_test)
real_classes_test <- as.numeric(fashion_mnist$test$y)

dt_pred_vs_real <- data.table(predicted = predicted_classes_test, real = real_classes_test)

library(ggplot2)
ggplot(dt_pred_vs_real[, .N, by = .(predicted, real)], aes(predicted, real)) +
  geom_tile(aes(fill = N), colour = "white") +
  scale_x_continuous(breaks = 0:9) +
  scale_y_continuous(breaks = 0:9) +
  geom_text(aes(label = sprintf("%1.0f", N)), vjust = 1, color = "white") +
  scale_fill_viridis_c() +
  theme_bw() + theme(legend.position = "none")

```

```{r}
class_names
```

As we can note, the algorithm performs really good when the classes are completely different, but tends to have a couple mistakes between t-shirt/top and shirt and coat, and sandalas and sneakers and ankle boots. Which even by human categorization it can also lead to some mistakes. 

A 90% accuracy on the test set is good enough to minimize these kind of categorization errors that would happen if a human or a non CNN model did this.

```{r}
dt_pred_vs_real[, row_number := 1:.N]
indices_of_mistakes <- dt_pred_vs_real[predicted != real][["row_number"]]
ix <- indices_of_mistakes[2]

dt_pred_vs_real[row_number == ix]
show_mnist_image(fashion_mnist$test$x[ix, , ])
```

This example shows an Ankle boot getting mistaken as a Sandal. 

### 2. Hot dog or not hot dog? (12 points)

In this problem you are going to predict if a certain image containing food is hot dog or is something else. Motivation for this comes from the comedy show Silicon Valley (see here).

The data can be found in the course repo and is originally downloaded from here.

- Pre-process data so that it is acceptable by Keras (set folder structure, bring images to the same size, etc).

```{r}
train_datagen = image_data_generator(
  rescale = 1/255,
  rotation_range = 40,
  width_shift_range = 0.2,
  height_shift_range = 0.2,
  shear_range = 0.2,
  zoom_range = 0.2,
  horizontal_flip = TRUE,
  fill_mode = "nearest"
)  

test_datagen <- image_data_generator(rescale = 1/255)  
```

```{r}
image_size <- c(150, 150)
batch_size <- 50

train_generator <- flow_images_from_directory(
  file.path(here(), "data/train/"), # Target directory  
  train_datagen,              # Data generator
  target_size = image_size,  # Resizes all images to 150 × 150
  batch_size = batch_size,
  class_mode = "binary"       # binary_crossentropy loss for binary labels
)


test_generator <- flow_images_from_directory(
  file.path(here(), "data/test/"), # Target directory  
  test_datagen,
  target_size = image_size,
  batch_size = batch_size,
  class_mode = "binary"
)
```

- Estimate a convolutional neural network to predict if an image contains a hot dog or not. Evaluate your model on the test set.

```{r}
hot_or_not_model <- keras_model_sequential() 
hot_or_not_model %>% 
  layer_conv_2d(filters = 32,
                kernel_size = c(3, 3), 
                activation = 'relu',
                input_shape = c(150, 150, 3)) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 16,
                kernel_size = c(3, 3), 
                activation = 'relu') %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_conv_2d(filters = 16,
                kernel_size = c(3, 3), 
                activation = 'relu') %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>% 
  layer_dropout(rate = 0.25) %>% 
  layer_flatten() %>% 
  layer_dense(units = 8, activation = 'relu') %>% 
  layer_dense(units = 1, activation = "sigmoid")   # for binary

hot_or_not_model %>% compile(
  loss = "binary_crossentropy",
  optimizer = optimizer_rmsprop(lr = 2e-5),
  metrics = c("accuracy")
)
```

```{r}

history <- hot_or_not_model %>% fit_generator(
  train_generator,
  steps_per_epoch = 2000 / batch_size,
  epochs = 30
)

```

- Could data augmentation techniques help with achieving higher predictive accuracy? Try some augmentations that you think make sense and compare

- Try to rely on some pre-built neural networks to aid prediction. Can you achieve a better performance using transfer learning for this problem?